{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all data and merge it in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:54:46.290201Z",
     "iopub.status.busy": "2024-12-04T07:54:46.289879Z",
     "iopub.status.idle": "2024-12-04T07:55:20.735830Z",
     "shell.execute_reply": "2024-12-04T07:55:20.735092Z",
     "shell.execute_reply.started": "2024-12-04T07:54:46.290172Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "emotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n",
    "\n",
    "# import and convert tweet (a bit slow, could be optimized)\n",
    "with open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate Train and test/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:55:20.737084Z",
     "iopub.status.busy": "2024-12-04T07:55:20.736810Z",
     "iopub.status.idle": "2024-12-04T07:55:23.701251Z",
     "shell.execute_reply": "2024-12-04T07:55:23.700312Z",
     "shell.execute_reply.started": "2024-12-04T07:55:20.737058Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "test_data = df[df['identification'] == 'test']\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:55:23.805884Z",
     "iopub.status.busy": "2024-12-04T07:55:23.805606Z",
     "iopub.status.idle": "2024-12-04T07:55:26.285228Z",
     "shell.execute_reply": "2024-12-04T07:55:26.284272Z",
     "shell.execute_reply.started": "2024-12-04T07:55:23.805857Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 0 # ATTENTION: NON-DETERMINISTIC\n",
    "TEST_SPLIT = 0.15\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data['text'],\n",
    "                                                    train_data['emotion'],\n",
    "                                                    test_size = TEST_SPLIT,\n",
    "                                                    random_state = RANDOM_STATE,\n",
    "                                                    stratify = train_data['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:55:26.287386Z",
     "iopub.status.busy": "2024-12-04T07:55:26.286900Z",
     "iopub.status.idle": "2024-12-04T07:55:26.521272Z",
     "shell.execute_reply": "2024-12-04T07:55:26.520486Z",
     "shell.execute_reply.started": "2024-12-04T07:55:26.287352Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights\n",
    "The dataset is somewhat imbalanced when it comes to the number of observations for each class, as such I decided to use class weights so that the models takes this into consideration.\n",
    "\n",
    "Using class weights with BERT helps avoid overfitting and keeps training efficient, whereas oversampling adds redundancy, is slower, and could harm the model's ability to generalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:55:26.522616Z",
     "iopub.status.busy": "2024-12-04T07:55:26.522353Z",
     "iopub.status.idle": "2024-12-04T07:55:30.207430Z",
     "shell.execute_reply": "2024-12-04T07:55:30.206665Z",
     "shell.execute_reply.started": "2024-12-04T07:55:26.522592Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# use class weights as it is less expensive than oversampling\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "class_weights_tensor = class_weights_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model\n",
    "Initialization of [BERTweet](https://github.com/VinAIResearch/BERTweet), a pre-trained model for english tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(y_train))  \n",
    "model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base',\n",
    "                                                           num_labels=num_labels\n",
    "                                                          )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Allow for dual GPU use\n",
    "#model = DDP(model) is recommended but is not working\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyTorch Dataset and Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:55:30.209235Z",
     "iopub.status.busy": "2024-12-04T07:55:30.208760Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128 # consider reducing in case of OOM\n",
    "NUM_WORKERS = 4 # 4 cores available in kaggle vm\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    " \n",
    "        # tokenize the entire dataset upfront\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx].squeeze(0),\n",
    "            'attention_mask': self.encodings['attention_mask'][idx].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', normalization=True) # trained on tweets\n",
    "max_len = 128 # max length of bertweet\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        PostDataset(pd.Series(X_train), y_train, tokenizer, max_len),\n",
    "        batch_size = BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS, pin_memory=True\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "        PostDataset(pd.Series(X_val), y_val, tokenizer, max_len),\n",
    "        batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "EPOCHS = 1 # Used in the final run because of time constraints, however loss remained rather constant throughout epochs\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor, reduction='median')\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=0.01)\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_training_steps = len(train_loader) * EPOCHS\n",
    "# Will increase the LR during warmup and then decrease it as it gets \"more tuned\"\n",
    "scheduler = get_scheduler(name=\"linear\",\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=int(0.1 * num_training_steps),\n",
    "                          num_training_steps=num_training_steps\n",
    "                         )\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, scaler, device, scheduler):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    # Loop through the DataLoader\n",
    "    for batch in data_loader:\n",
    "        # load onto GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(torch.long).to(device) # Make sure it's long was causing an issue\n",
    "\n",
    "        # Zero the gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with autocast for mixed precision\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # avoid 2d error\n",
    "        if loss.dim() > 0:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / num_batches\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient calculations during evaluation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # load onto GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(torch.long).to(device)  # Ensure labels are torch.long\n",
    "        \n",
    "            # Forward pass with autocast for mixed precision\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # avoid 2d error\n",
    "            if loss.dim() > 0:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            running_loss += loss\n",
    "\n",
    "            # Predict\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate the average loss\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "\n",
    "    # Calculate F1 score (weighted average)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return average_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 2\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Train and validate\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device, scheduler)\n",
    "    test_loss, f1 = eval_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Save best model and stop early to prevent overfitting\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Generate Results / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    PostDataset(test_data['text'], [0] * len(test_data), tokenizer, max_len),\n",
    "    batch_size=64, num_workers = NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=num_labels)\n",
    "\n",
    "# Wrap the model in DataParallel\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Load the state_dict\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # load to gpu\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Predict\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# retrieve original labels and output\n",
    "test_data['emotion'] = le.inverse_transform(test_predictions)\n",
    "submission = test_data[['tweet_id', 'emotion']]\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print('Submission saved successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Observations\n",
    "\n",
    "This model is highly computationally intensive and could benefit from more optimizations.\n",
    "I tried implementing it to use TPU, but there was a bug with the transformers library causing the environment to crash.\n",
    "\n",
    "To improve the model more data could be generated, for example by using word2vec to replace words with synonyms or use LLMs to rephrase sentences. This would be an expensive task but that could prove beneficial.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9912598,
     "sourceId": 87232,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
